<div class="projects-section">
    <center><h2>Research projects</h2><center>
    <ul class="projects-list wrapper">
        <!-- Primate-like decision making -->
        <li class="project-item">
        <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/arm_slow.gif" alt="Primate-like decision making in deep RL" class="project-image" width="50%">
        </div>
        <div class="project-content">
            <div class="project-title">
                <h3>Primate-like perceptual decision making emerges through deep recurrent reinforcement learning</h3>
            </div>
            <p class="project-authors"><strong class="author-me">Nathan Wispinski</strong>, Scott Stone, Anthony Singhal, Patrick Pilarski, & Craig Chapman</p>
            <p class="project-venue"><i>arXiv </i>(2026)</p>
            <hr>
            <p class="project-desc">
                We show that human-like movements and decision-making abilities emerge in deep neural networks trained using reinforcement learning.
                But only with the right ingredients—some theory from neuroscience and evolutionary biology.
            </p>
            <div class="project-links">
                <nobr><a class="project-link" href="https://arxiv.org/abs/2601.12577" target="_blank">
                    <i class="ai fa-fw fa-lg ai-arxiv"></i>
                    Preprint
                </a></nobr>
            </div>
        </div>
        </li>
        <!-- Where do we put the body? -->
        <li class="project-item">
        <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/putthebody.png" alt="Where do we put the body?" class="project-image" width="50%">
        </div>
        <div class="project-content">
            <div class="project-title">
                <h3>Hold it! Where do we put the body?</h3>
            </div>
            <p class="project-authors"><strong class="author-me">Nathan Wispinski</strong>, James Enns, & Craig Chapman</p>
            <p class="project-venue"><i>Behavioral and Brain Sciences </i>(2023)</p>
            <hr>
            <p class="project-desc">
                We argue that for a full understanding of the psychology of ownership, we need to consider humans as embodied agents.
            </p>
            <div class="project-links">
                <nobr><a class="project-link" href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/hold-it-where-do-we-put-the-body/E62BBD914D222C13EFCAF338042256FB" target="_blank">
                    <i class="fa fa-fw fa-file"></i>
                    Paper
                </a></nobr>
            </div>
        </div>
        </li>

        <!-- Adaptive patch foraging -->
        <li class="project-item">
            <div class="project-image-container">
                    <img class="project-image" src="/assets/images/project-thumbnails/foraging.gif" alt="Adaptive patch foraging in deep RL" class="project-image" width="50%">
            </div>
            <div class="project-content">
                <div class="project-title">
                    <h3>Adaptive patch foraging in deep reinforcement learning agents</h3>
                </div>
                <p class="project-authors"><strong class="author-me">Nathan Wispinski</strong>, Andrew Butcher, Kory Mathewson, Craig Chapman, Matthew Botvinick, & Patrick Pilarski</p>
                <p class="project-venue"><i>Transactions on Machine Learning Research </i>(2023)</p>
                <hr>
                <p class="project-desc">
                    During my time at <a href="https://deepmind.com/" target="_blank">Google DeepMind</a>, we trained deep neural networks to forage.
                    These artificial agents learned to forage adaptively like many animals. They also learned neural patterns that looked like those recorded from foraging primates.
                </p>
                <div class="project-links">
                    <nobr><a class="project-link" href="https://openreview.net/pdf?id=a0T3nOP9sB" target="_blank">
                        <i class="fa fa-fw fa-file"></i>
                        Paper
                    </a></nobr>
                    <nobr><a class="project-link" href="https://arxiv.org/abs/2210.08085" target="_blank">
                        <i class="ai fa-fw fa-lg ai-arxiv"></i>
                        Preprint
                    </a></nobr>
                    <nobr><a class="project-link" href="https://youtu.be/5PfxPj5Jzwo" target="_blank">
                        <i class="fa fa-fw fa-video-camera"></i>
                        Videos
                    </a></nobr>
                    <nobr><a class="project-link" href="https://www.youtube.com/watch?v=k6bD5W5C0h4" target="_blank">
                        <i class="fa fa-fw fa-video-camera"></i>
                        Talk
                    </a></nobr>
                </div>
            </div>
        </li>
        <!-- Meta-RL -->
        <li class="project-item">
        <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/meta-rl-network.png" alt="Meta-reinforcement learning" class="project-image" width="50%">
        </div>
        <div class="project-content">
            <div class="project-title">
                <h3>Meta-reinforcement learning</h3>
            </div>
            <p class="project-authors"><strong class="author-me">Nathan Wispinski</strong></p>
            <!-- <p class="project-venue">(In progress)</p> -->
            <hr>
            <p class="project-desc">
                A short, conceptual replication of <i>Prefrontal cortex as a meta-reinforcement learning system</i> in the <a href="https://github.com/google/jax" target="_blank">Jax ecosystem</a>.
            </p>
            <div class="project-links">
                <nobr><a class="project-link" href="https://github.com/nathanwispinski/meta-rl" target="_blank">
                    <i class="fa fa-fw fa-github"></i>
                    GitHub
                </a></nobr>
            </div>
        </div>
        </li>
        <!-- Delayed Reach Retinotopy -->
        <li class="project-item">
        <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/mri.gif" alt="Delayed reaching retinotopy in EEG" class="project-image" width="50%">
        </div>
        <div class="project-content">
            <div class="project-title">
                <h3>Delayed reaching retinotopy</h3>
            </div>
            <p class="project-authors"><strong class="author-me">Nathan Wispinski</strong>, Scott Stone, Craig Chapman, & Anthony Singhal</p>
            <!-- <p class="project-venue">(In progress)</p> -->
            <hr>
            <p class="project-desc">
                We simultaneously used motion tracking, eye tracking, and high-density EEG to see how the brain reactivates neural patterns to make actions.
            </p>
            <div class="project-links">
                <nobr><a class="project-link" href="/delayedreaching.html">
                    <i class="fa fa-fw fa-link"></i>
                    Blog
                </a></nobr>
            </div>
        </div>
        </li>
        <!-- Reaching for the known unknowns -->
        <li class="project-item">
        <div class="project-image-container">
            <img class="project-image" src="/assets/images/project-thumbnails/reachingforknownunknowns_thumbnail.png" alt="Reaching for the known unknowns: Rapid reach decisions accurately reflect the future state of dynamic probabilistic information" class="project-image" width="50%">
        </div>
        <div class="project-content">
            <div class="project-title">
                <h3>Reaching for the known unknowns: Rapid reach decisions accurately reflect the future state of dynamic probabilistic information</h3>
            </div>
            <p class="project-authors"><strong class="author-me">Nathan Wispinski</strong>, Scott Stone, Jennifer Bertrand, Alexandra Ouellette Zuk, Ewen Lavoie, Jason Gallivan, & Craig Chapman</p>
            <p class="project-venue"><i>Cortex </i>(2021)</p>
            <hr>
            <p class="project-desc">
                By tracking high-speed reaching movements, we show that humans use predictions to optimally guide their actions.
            </p>
            <div class="project-links">
                <nobr><a class="project-link" href="https://www.sciencedirect.com/science/article/abs/pii/S001094522100068X" target="_blank">
                    <i class="fa fa-fw fa-file"></i>
                    Paper
                </a></nobr>
                <nobr><a class="project-link" href="https://www.biorxiv.org/content/10.1101/2020.07.31.231563v1.full.pdf" target="_blank">
                    <i class="ai fa-fw fa-lg ai-biorxiv"></i>
                    Preprint
                </a></nobr>
                <nobr><a class="project-link" href="https://osf.io/rt5xv/" target="_blank">
                    <i class="ai fa-fw ai-open-materials"></i>
                    OSF
                </a></nobr>
                <nobr><a class="project-link" href="https://github.com/nathanwispinski/wispinski-cortex-2021" target="_blank">
                    <i class="fa fa-fw fa-github"></i>
                    GitHub
                </a></nobr>
                <nobr><a class="project-link" href="/cortex2021.html">
                    <i class="fa fa-fw fa-link"></i>
                    Blog
                </a></nobr>
            </div>
        </div>
        </li>
        <!-- Selective attention -->
        <li class="project-item">
            <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/selectiveattention.gif" alt="Selective attention to real world objects drives their emotional appraisal" class="project-image" width="50%">
            </div>
            <div class="project-content">
                <div class="project-title">
                    <h3>Selective attention to real world objects drives their emotional appraisal</h3>
                </div>
                <p class="project-authors"><strong class="author-me">Nathan Wispinski</strong>, Shihao Lin, James Enns, & Craig Chapman</p>
                <p class="project-venue"><i>Attention, Perception, & Psychophysics </i>(2021)</p>
                <hr>
                <p class="project-desc">
                    We show that simply attending to 3D objects makes you like those objects more.
                </p>
                <div class="project-links">
                    <nobr><a class="project-link" href="https://rdcu.be/caYTx" target="_blank">
                        <i class="fa fa-fw fa-file"></i>
                        Paper
                    </a></nobr>
                    <nobr><a class="project-link" href="https://osf.io/iyd9s/" target="_blank">
                        <i class="ai fa-fw ai-open-materials"></i>
                        OSF
                    </a></nobr>
                    <nobr><a class="project-link" href="https://github.com/nathanwispinski/wispinski-app-2021" target="_blank">
                        <i class="fa fa-fw fa-github"></i>
                        GitHub
                    </a></nobr>
                    <nobr><a class="project-link" href="/selectiveattention2021.html">
                        <i class="fa fa-fw fa-link"></i>
                        Blog
                    </a></nobr>
                </div>
            </div>
            </li>
        <!-- Models, movements, and minds -->
        <li class="project-item">
            <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/modelsmovementsminds_thumbnail.png" alt="Models, movements, and minds: Bridging the gap between decision making and action" class="project-image" width="50%">
            </div>
            <div class="project-content">
                <div class="project-title">
                    <h3>Models, movements, and minds: Bridging the gap between decision making and action</h3>
                </div>
                <p class="project-authors"><strong class="author-me">Nathan Wispinski</strong>, Jason Gallivan, & Craig Chapman</p>
                <p class="project-venue"><i>Annals of the New York Academy of Sciences </i>[The Year in Cognitive Neuroscience series] (2018)</p>
                <hr>
                <p class="project-desc">
                    In this review, we argue that decision making and motor control are much more intimately linked in the brain than many researchers think.
                </p>
                <div class="project-links">
                    <nobr><a class="project-link" href="/assets/models-movements-minds-wispinski-etal-2018.pdf">
                        <i class="fa fa-fw fa-file"></i>
                        Paper
                    </a></nobr>
                </div>
            </div>
            </li>
        <!-- Situated cognition -->
        <li class="project-item">
            <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/comparativecognition_thumbnail.png" alt="Examining the “species” of situated cognition in humans" class="project-image" width="50%">
            </div>
            <div class="project-content">
                <div class="project-title">
                    <h3>Examining the “species” of situated cognition in humans</h3>
                </div>
                <p class="project-authors">Ewen Lavoie, Jennifer Bertrand, Scott Stone, <strong class="author-me">Nathan Wispinski</strong>, Jeff Sawalha, & Craig Chapman</p>
                <p class="project-venue"><i>Comparative Cognition & Behavior Reviews </i>(2018)</p>
                <hr>
                <p class="project-desc">
                    In this review, we discuss distributed, extended, and embodied cognition in humans.
                </p>
                <div class="project-links">
                    <nobr><a class="project-link" href="http://comparative-cognition-and-behavior-reviews.org/wp/wp-content/uploads/2018/04/CCBR-vol13-2018-pp031-034-Lavoie-Betrand-et-al.pdf" target="_blank">
                        <i class="fa fa-fw fa-file"></i>
                        Paper
                    </a></nobr>
                </div>
            </div>
            </li>
        <!-- Brightness enhancement -->
        <li class="project-item">
            <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/burke-effect.gif" alt="Entrainment of theta, not alpha, oscillations is predictive of the brightness enhancement of a flickering stimulus" class="project-image" width="50%">
            </div>
            <div class="project-content">
                <div class="project-title">
                    <h3>Entrainment of theta, not alpha, oscillations is predictive of the brightness enhancement of a flickering stimulus</h3>
                </div>
                <p class="project-authors">Jennifer Bertrand, <strong class="author-me">Nathan Wispinski</strong>, Kyle Mathewson, & Craig Chapman</p>
                <p class="project-venue"><i>Scientific Reports </i>(2018)</p>
                <hr>
                <p class="project-desc">
                    In 1864, Br&uuml;cke found that lights flickering at specific frequencies seemed brighter than others. 
                    In 2018, we showed that this strange effect is related to neural oscillations associated with information transfer.
                </p>
                <div class="project-links">
                    <nobr><a class="project-link" href="https://www.nature.com/articles/s41598-018-24215-3" target="_blank">
                        <i class="fa fa-fw fa-file"></i>
                        Paper
                    </a></nobr>
                </div>
            </div>
            </li>
        <!-- Best vs rest -->
        <li class="project-item">
            <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/bestvsrest_thumbnail.png" alt="Reaching reveals that best-versus-rest processing contributes to biased decision making" class="project-image" width="50%">
            </div>
            <div class="project-content">
                <div class="project-title">
                    <h3>Reaching reveals that best-versus-rest processing contributes to biased decision making</h3>
                </div>
                <p class="project-authors"><strong class="author-me">Nathan Wispinski</strong>, Grace Truong, Todd Handy, & Craig Chapman</p>
                <p class="project-venue"><i>Acta Psychologica </i>(2017)</p>
                <hr>
                <p class="project-desc">
                    We show that humans are biased toward the "best" option much more than classic economic theory predicts.
                </p>
                <div class="project-links">
                    <nobr><a class="project-link" href="https://www.sciencedirect.com/science/article/abs/pii/S0001691816302153" target="_blank">
                        <i class="fa fa-fw fa-file"></i>
                        Paper
                    </a></nobr>
                </div>
            </div>
            </li>
        <!-- Taxation -->
        <li class="project-item">
            <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/taxation_thumbnail.png" alt="Seeing wealth as a responsibility improves attitudes towards taxation" class="project-image" width="50%">
            </div>
            <div class="project-content">
                <div class="project-title">
                    <h3>Seeing wealth as a responsibility improves attitudes towards taxation</h3>
                </div>
                <p class="project-authors">Ashley Whillans, <strong class="author-me">Nathan Wispinski</strong>, & Elizabeth Dunn</p>
                <p class="project-venue"><i>Journal of Economic Behavior & Organization </i>(2016)</p>
                <hr>
                <p class="project-desc">
                    Studies show that people <i>disproportionaly</i> dislike taxation. In the lab, we let undergraduates earn money before taxing this income. 
                    Using some psychological priming, we found a way for taxation to feel less painful.
                </p>
                <div class="project-links">
                    <nobr><a class="project-link" href="https://www.sciencedirect.com/science/article/abs/pii/S016726811630049X" target="_blank">
                        <i class="fa fa-fw fa-file"></i>
                        Paper
                    </a></nobr>
                </div>
            </div>
            </li>
        <!-- Snooze of lose -->
        <li class="project-item">
            <div class="project-image-container">
                <img class="project-image" src="/assets/images/project-thumbnails/snoozeoflose_thumbnail.png" alt="The snooze of lose: Rapid reaching reveals that losses are processed more slowly than gains" class="project-image" width="50%">
            </div>
            <div class="project-content">
                <div class="project-title">
                    <h3>The snooze of lose: Rapid reaching reveals that losses are processed more slowly than gains</h3>
                </div>
                <p class="project-authors">Craig Chapman, Jason Gallivan, Jeremy Wong, <strong class="author-me">Nathan Wispinski</strong>, & James Enns</p>
                <p class="project-venue"><i>Journal of Experimental Psychology: General </i>(2015)</p>
                <hr>
                <p class="project-desc">
                    Using motion tracking when people make decisions under extreme time pressure, we show that humans process rewards faster than losses.
                </p>
                <div class="project-links">
                    <nobr><a class="project-link" href="https://visionlab-psych.sites.olt.ubc.ca/files/2016/12/164_Chapman_etal_JEPG15.pdf" target="_blank">
                        <i class="fa fa-fw fa-file"></i>
                        Paper
                    </a></nobr>
                </div>
            </div>
            </li>
        </ul>
    </div>
